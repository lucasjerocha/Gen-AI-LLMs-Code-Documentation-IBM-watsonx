# Codebase for IBM watsonx | RAG Pipelines | Prompt Tuning | Model Benchmarking

# Description:
This repository contains comprehensive code documentation supporting the Master's thesis titled: "GENERATIVE ARTIFICIAL INTELLIGENCE FOR STAKEHOLDER MANAGEMENT IN PROJECTS: SPECIALIZING A LARGE LANGUAGE MODEL". The code covers systematic benchmarking of large language models (LLMs), retrieval-augmented generation (RAG) pipelines, automated prompt tuning, and evaluation methods using the IBM watsonx.ai platform. It is designed to ensure full methodological transparency, reproducibility, and practical implementation guidelines.

# Main Contents:

Python scripts and Jupyter notebooks for model benchmarking (e.g., LLaMA-4 Maverick, Mistral Large, IBM Granite).

Prompt tuning scripts and configurations (hyperparameters, learning rates, epochs).

RAG pipeline code for specialized stakeholder management queries.

Evaluation and inference routines, including embedding and retrieval techniques.

# Usage:
This repository is ideal for researchers, practitioners, and students interested in applying advanced AI techniques to stakeholder management or similar knowledge-intensive domains. Detailed instructions and documentation are included to facilitate replication and extension.

# Keywords:
Stakeholder Management, LLMs, Prompt Tuning, Retrieval-Augmented Generation, IBM Watsonx.ai, Benchmarking, AI Ethics, Model Specialization
